{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgambone3/CSC4850-Machine-Learning/blob/main/FDS_Project_temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WnqpVkQOgpyZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to import data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgmDdwzauiLm",
        "outputId": "ed9a9328-36c5-4162-daa7-02b0c49e2c16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset and drop null column\n",
        "df = pd.read_csv('speeddating.csv')\n",
        "df = df.drop(labels=['has_null', 'field'], axis=1)\n",
        "np.seterr(divide='ignore', invalid='ignore') # ignore division error warnings\n",
        "\n",
        "# for col in df:\n",
        "#   print(col)\n",
        "\n",
        "df.replace(\"b\\'?\\'\", np.nan, inplace=True)\n",
        "\n",
        "### encoding object row values ###\n",
        "# output old value and encoded value for intrepretation later\n",
        "for col in df.columns:\n",
        "  if df[col].dtype == \"object\":\n",
        "    print(col)\n",
        "    old_values = pd.DataFrame(columns=[]) #dataframe to hold and display values\n",
        "    encoder = OrdinalEncoder()\n",
        "    df[col] = encoder.fit_transform(df[[col]]) #fit transform data to encoded data\n",
        "    # add to df\n",
        "    old_values['encoded_value'] = pd.DataFrame(df[col].unique())\n",
        "    old_values['original_value'] = pd.Series(encoder.categories_[0])\n",
        "    # print('\\n   ---',col,'---')\n",
        "    # print(old_values.sort_values(by=['encoded_value']))"
      ],
      "metadata": {
        "id": "2_x-OGM_i3_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37af058-ff99-460e-a003-942ec6c2249c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gender\n",
            "d_d_age\n",
            "race\n",
            "race_o\n",
            "samerace\n",
            "d_importance_same_race\n",
            "d_importance_same_religion\n",
            "d_pref_o_attractive\n",
            "d_pref_o_sincere\n",
            "d_pref_o_intelligence\n",
            "d_pref_o_funny\n",
            "d_pref_o_ambitious\n",
            "d_pref_o_shared_interests\n",
            "d_attractive_o\n",
            "d_sinsere_o\n",
            "d_intelligence_o\n",
            "d_funny_o\n",
            "d_ambitous_o\n",
            "d_shared_interests_o\n",
            "d_attractive_important\n",
            "d_sincere_important\n",
            "d_intellicence_important\n",
            "d_funny_important\n",
            "d_ambtition_important\n",
            "d_shared_interests_important\n",
            "d_attractive\n",
            "d_sincere\n",
            "d_intelligence\n",
            "d_funny\n",
            "d_ambition\n",
            "d_attractive_partner\n",
            "d_sincere_partner\n",
            "d_intelligence_partner\n",
            "d_funny_partner\n",
            "d_ambition_partner\n",
            "d_shared_interests_partner\n",
            "d_sports\n",
            "d_tvsports\n",
            "d_exercise\n",
            "d_dining\n",
            "d_museums\n",
            "d_art\n",
            "d_hiking\n",
            "d_gaming\n",
            "d_clubbing\n",
            "d_reading\n",
            "d_tv\n",
            "d_theater\n",
            "d_movies\n",
            "d_concerts\n",
            "d_music\n",
            "d_shopping\n",
            "d_yoga\n",
            "d_interests_correlate\n",
            "d_expected_happy_with_sd_people\n",
            "d_expected_num_interested_in_me\n",
            "d_expected_num_matches\n",
            "d_like\n",
            "d_guess_prob_liked\n",
            "decision\n",
            "decision_o\n",
            "match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "FktazFgopGFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# impute null values - dropping all null would significantly reduce dataset size\n",
        "null = df.isnull().sum() # find amount all null values\n",
        "print('Data shape before dropping null: ', df.shape)\n",
        "# print(null)\n",
        "# drop cols with more than 300 null values\n",
        "drop_cols = [col for col, null_count in null.items() if null_count > 300]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "null = df.isnull().sum()\n",
        "print('Data shape after dropping null: ', df.shape)\n",
        "# print(null)\n",
        "\n",
        "# impute null values\n",
        "for column, null_count in null.items():\n",
        "  if null_count != 0:\n",
        "    med = df[column].median()\n",
        "    df[column].fillna(med, inplace=True)\n",
        "\n",
        "\n",
        "XX = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(XX, y, train_size=0.8, test_size=0.2, random_state=1234)\n",
        "\n",
        "# best = SelectKBest(f_classif, k=20).set_output(transform='pandas')\n",
        "# X_best = best.fit_transform(XX, y)\n",
        "# feat_scores = pd.DataFrame(columns=XX.columns)\n",
        "# scores = pd.DataFrame(best.scores_, columns=['Score'], index=XX.columns)\\\n",
        "#                       .sort_values(by=['Score'], ascending=False)\\\n",
        "#                       .reset_index(names=['feature'])\n",
        "# print(scores)\n",
        "# print(X_best.columns)\n",
        "# X = X_best\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(XX).transform(XX)\n",
        "print((pca.explained_variance_ratio_))\n",
        "pcs = pca.components_.shape[0]\n",
        "most_important = [np.abs(pca.components_[i]).argmax() for i in range(pcs)]\n",
        "init_cols = XX.columns\n",
        "pc_names = [init_cols[most_important[i]] for i in range(pcs)]\n",
        "dic = {'PC{}'.format(i): pc_names[i] for i in range(pcs)}\n",
        "pca_cols = pd.DataFrame(dic.items())\n",
        "print(pca_cols)\n",
        "# corr = round(X.corr(), 2)\n",
        "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# sns.heatmap(corr, vmin=0, vmax=1, cmap='GnBu')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "O12Kz8m_uZCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8474d22-bff9-4b18-f730-a9073c8f1663"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape before dropping null:  (8378, 110)\n",
            "Data shape after dropping null:  (8378, 110)\n",
            "[0.20875272 0.18418284 0.05578497 0.05490734 0.0513087  0.0466328\n",
            " 0.04428267 0.04272251 0.03977513 0.03542304 0.02888487 0.02462835\n",
            " 0.01851272 0.01590252 0.01287605 0.01188949 0.01067564 0.00953817\n",
            " 0.00861802 0.00775471 0.00684318 0.00620328 0.00550472 0.0051466\n",
            " 0.00473504 0.00446023 0.00411358 0.0039547  0.00365697 0.0031089 ]\n",
            "       0                         1\n",
            "0    PC0      attractive_important\n",
            "1    PC1         pref_o_attractive\n",
            "2    PC2    intellicence_important\n",
            "3    PC3       pref_o_intelligence\n",
            "4    PC4            pref_o_sincere\n",
            "5    PC5           funny_important\n",
            "6    PC6              pref_o_funny\n",
            "7    PC7       pref_o_intelligence\n",
            "8    PC8                      wave\n",
            "9    PC9       ambtition_important\n",
            "10  PC10          pref_o_ambitious\n",
            "11  PC11                     d_age\n",
            "12  PC12                       art\n",
            "13  PC13                  tvsports\n",
            "14  PC14                    sports\n",
            "15  PC15                     age_o\n",
            "16  PC16                     age_o\n",
            "17  PC17                      like\n",
            "18  PC18                       age\n",
            "19  PC19                    gaming\n",
            "20  PC20                      yoga\n",
            "21  PC21                  clubbing\n",
            "22  PC22  importance_same_religion\n",
            "23  PC23              attractive_o\n",
            "24  PC24                     music\n",
            "25  PC25                   reading\n",
            "26  PC26                  exercise\n",
            "27  PC27      importance_same_race\n",
            "28  PC28                    hiking\n",
            "29  PC29                   reading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sM38vLzLu4Dk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}